# ğŸš€ Event-Driven Agentic Document Workflows

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![LlamaIndex](https://img.shields.io/badge/LlamaIndex-0.12.42-green.svg)](https://www.llamaindex.ai/)
[![Google Gemini](https://img.shields.io/badge/Google%20Gemini-2.0%20Flash-orange.svg)](https://ai.google.dev/)
[![VOSK](https://img.shields.io/badge/VOSK-0.3.45-red.svg)](https://alphacephei.com/vosk/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebooks-orange.svg)](https://jupyter.org/)

> ğŸ¤– **Intelligent document processing workflows with human-in-the-loop feedback, voice interaction, and RAG-powered automation using Google Gemini and offline speech recognition.**

## ğŸ“‹ Table of Contents

- [ğŸŒŸ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“š Notebooks](#-notebooks)
- [ğŸ”§ Installation](#-installation)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ“– Usage Examples](#-usage-examples)
- [ğŸ¯ Workflow Types](#-workflow-types)
- [ğŸ”Š Voice Integration](#-voice-integration)
- [ğŸ“Š RAG Implementation](#-rag-implementation)
- [ğŸ¤ Human-in-the-Loop](#-human-in-the-loop)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ› ï¸ Technologies](#ï¸-technologies)
- [ğŸ“ˆ Performance](#-performance)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

## ğŸŒŸ Features

### ğŸ¯ **Core Capabilities**
- **Event-Driven Architecture**: Asynchronous workflow orchestration with custom events
- **Document Intelligence**: Advanced PDF parsing with LlamaParse and RAG capabilities
- **Human-in-the-Loop**: Interactive feedback loops for quality assurance
- **Voice Integration**: Offline speech recognition with VOSK (no API keys required)
- **Multi-Model Support**: Google Gemini 3 27B for LLM operations
- **Workflow Visualization**: Interactive HTML diagrams for workflow understanding

### ğŸ”¥ **Advanced Features**
- **Intelligent Form Filling**: Automated application form completion from resumes
- **Context-Aware Responses**: Vector-based document retrieval and generation
- **Feedback Integration**: Dynamic workflow adaptation based on human input
- **Offline Speech Recognition**: Complete privacy with local VOSK processing
- **Concurrent Processing**: Parallel event handling for optimal performance
- **Error Recovery**: Robust error handling and fallback mechanisms

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[ğŸ“„ Document Input] --> B[ğŸ” LlamaParse]
    B --> C[ğŸ§  Vector Store Index]
    C --> D[âš¡ Event-Driven Workflow]
    
    D --> E[ğŸ¯ Query Generation]
    E --> F[ğŸ¤– Gemini LLM Processing]
    F --> G[ğŸ“ Response Generation]
    
    G --> H{ğŸ¤” Human Review}
    H -->|âœ… Approve| I[âœ¨ Final Output]
    H -->|ğŸ”„ Feedback| J[ğŸ”§ Workflow Adaptation]
    J --> E
    
    K[ğŸ¤ Voice Input] --> L[ğŸ—£ï¸ VOSK Recognition]
    L --> H
    
    style A fill:#e1f5fe
    style I fill:#e8f5e8
    style H fill:#fff3e0
    style K fill:#f3e5f5
```

## ğŸš€ Quick Start

### 1ï¸âƒ£ **Clone & Setup**
```bash
git clone https://github.com/jagadeshchilla/Event-Driven-Agentic-Document-Workflows.git
cd Event-Driven-Agentic-Document-Workflows
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ **Environment Configuration**
```bash
# Create .env file
echo "GEMINI_API_KEY=your_gemini_api_key_here" > .env
echo "LLAMA_CLOUD_API_KEY=your_llama_cloud_api_key_here" >> .env
```

### 3ï¸âƒ£ **Launch Jupyter**
```bash
jupyter notebook
```

### 4ï¸âƒ£ **Start with Basic Workflow**
Open `Workflow.ipynb` and run the cells to see the magic happen! âœ¨

## ğŸ“š Notebooks

| Notebook | Description | Key Features |
|----------|-------------|--------------|
| ğŸ”§ **Workflow.ipynb** | Core workflow implementation | Event-driven architecture, custom events, visualization |
| ğŸ“„ **Adding_Rag.ipynb** | RAG implementation | Document parsing, vector indexing, query engines |
| ğŸ”„ **human_in_loop.ipynb** | Human feedback integration | Interactive workflows, feedback loops, quality control |
| ğŸ¤ **use_your_voice.ipynb** | Voice-enabled workflows | VOSK speech recognition, audio transcription, voice feedback |
| ğŸ“‹ **former_parsing.ipynb** | Document parsing utilities | PDF processing, form extraction, data structuring |

## ğŸ”§ Installation

### ğŸ“‹ **Prerequisites**
- Python 3.8+
- Jupyter Notebook
- Google Gemini API Key
- LlamaCloud API Key (free tier available)

### ğŸ› ï¸ **Dependencies**
```bash
# Core dependencies
pip install llama-index-core
pip install llama-index-llms-gemini
pip install llama-index-embeddings-gemini
pip install llama-parse
pip install google-generativeai

# Voice processing
pip install vosk

# UI and visualization
pip install gradio
pip install ipykernel

# Utilities
pip install python-dotenv
```

### ğŸ¯ **One-Command Install**
```bash
pip install -r requirements.txt
```

## âš™ï¸ Configuration

### ğŸ”‘ **API Keys Setup**
1. **Google Gemini API**: Get your key from [Google AI Studio](https://ai.google.dev/)
2. **LlamaCloud API**: Free key from [LlamaIndex Cloud](https://cloud.llamaindex.ai/)

### ğŸ“ **Environment Variables**
```env
# .env file
GEMINI_API_KEY=your_gemini_api_key_here
LLAMA_CLOUD_API_KEY=your_llama_cloud_api_key_here
LLAMA_CLOUD_BASE_URL=https://api.cloud.llamaindex.ai
```

### ğŸ›ï¸ **Model Configuration**
```python
# Gemini Model Settings
MODEL_NAME = "models/gemma-3-27b-it"
EMBEDDING_MODEL = "models/text-embedding-004"
TEMPERATURE = 0.7
MAX_TOKENS = 4096
```

## ğŸ“– Usage Examples

### ğŸ”„ **Basic Workflow**
```python
from llama_index.core.workflow import Workflow, StartEvent, StopEvent

class MyWorkflow(Workflow):
    @step
    async def process(self, ev: StartEvent) -> StopEvent:
        return StopEvent(result="Workflow completed!")

# Run the workflow
workflow = MyWorkflow(timeout=60)
result = await workflow.run()
print(result)  # Output: Workflow completed!
```

### ğŸ“„ **Document Processing**
```python
from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex

# Parse document
documents = LlamaParse(
    api_key=llama_cloud_api_key,
    result_type="markdown"
).load_data("data/resume.pdf")

# Create searchable index
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

# Query the document
response = query_engine.query("What are the candidate's technical skills?")
print(response)
```

### ğŸ¤ **Voice Integration**
```python
import vosk
import wave

def transcribe_audio(audio_file):
    model = vosk.Model("vosk-model-small-en-us-0.15")
    rec = vosk.KaldiRecognizer(model, 16000)
    
    wf = wave.open(audio_file, 'rb')
    # Process audio and return transcription
    return transcription
```

## ğŸ¯ Workflow Types

### ğŸ“Š **1. Linear Workflows**
- Sequential step execution
- Simple event flow
- Perfect for straightforward tasks

### ğŸŒ **2. Branching Workflows**
- Conditional logic
- Multiple execution paths
- Dynamic decision making

### ğŸ”„ **3. Loop Workflows**
- Iterative processing
- Feedback incorporation
- Continuous improvement

### âš¡ **4. Concurrent Workflows**
- Parallel processing
- Event multiplexing
- High-performance execution

## ğŸ”Š Voice Integration

### ğŸ™ï¸ **VOSK Speech Recognition**
- **Offline Processing**: No internet required after model download
- **Privacy First**: All processing happens locally
- **Multi-language Support**: 20+ languages supported
- **Real-time Recognition**: Streaming audio processing

### ğŸ”§ **Setup Voice Recognition**
```python
# VOSK model downloads automatically on first use
# Model size: ~50MB for English
# Supports: WAV format (mono, 16kHz, 16-bit)

def setup_voice_recognition():
    model_path = "vosk-model-small-en-us-0.15"
    if not os.path.exists(model_path):
        # Model downloads automatically
        print("Downloading VOSK model...")
    
    model = vosk.Model(model_path)
    recognizer = vosk.KaldiRecognizer(model, 16000)
    return recognizer
```

## ğŸ“Š RAG Implementation

### ğŸ§  **Vector Store Architecture**
```python
# Document Processing Pipeline
Document â†’ LlamaParse â†’ Chunks â†’ Embeddings â†’ Vector Store â†’ Query Engine
```

### ğŸ” **Retrieval Process**
1. **Document Ingestion**: PDF parsing with LlamaParse
2. **Chunking**: Intelligent text segmentation
3. **Embedding**: Google Gemini text-embedding-004
4. **Indexing**: Vector store creation
5. **Retrieval**: Similarity-based search
6. **Generation**: Context-aware responses

### ğŸ“ˆ **Performance Metrics**
- **Embedding Dimension**: 768
- **Similarity Top-K**: 5
- **Context Window**: 8192 tokens
- **Response Time**: <2 seconds average

## ğŸ¤ Human-in-the-Loop

### ğŸ”„ **Feedback Loop Architecture**
```python
class HumanFeedbackWorkflow(Workflow):
    @step
    async def process_document(self, ev: StartEvent) -> InputRequiredEvent:
        # Process document
        result = self.llm.complete(query)
        
        # Request human feedback
        return InputRequiredEvent(
            prefix="Please review this response:",
            result=result
        )
    
    @step
    async def incorporate_feedback(self, ev: HumanResponseEvent) -> StopEvent:
        # Process human feedback
        if self.should_continue(ev.response):
            return StopEvent(result=final_result)
        else:
            return FeedbackEvent(feedback=ev.response)
```

### ğŸ¯ **Feedback Types**
- **Approval**: Accept generated content
- **Correction**: Provide specific improvements
- **Rejection**: Request complete regeneration
- **Guidance**: Offer directional feedback

## ğŸ“ Project Structure

```
Event-Driven-Agentic-Document-Workflows/
â”œâ”€â”€ ğŸ“š notebooks/
â”‚   â”œâ”€â”€ Workflow.ipynb              # Core workflow implementation
â”‚   â”œâ”€â”€ Adding_Rag.ipynb           # RAG implementation
â”‚   â”œâ”€â”€ human_in_loop.ipynb        # Human feedback integration
â”‚   â”œâ”€â”€ use_your_voice.ipynb       # Voice recognition
â”‚   â””â”€â”€ former_parsing.ipynb       # Document parsing
â”œâ”€â”€ ğŸ“„ data/
â”‚   â”œâ”€â”€ fake_resume.pdf            # Sample resume
â”‚   â”œâ”€â”€ fake_application_form.pdf  # Sample application form
â”‚   â””â”€â”€ cv1.pdf                    # Additional CV sample
â”œâ”€â”€ ğŸ¨ workflows/
â”‚   â”œâ”€â”€ basic_workflow.html        # Workflow visualizations
â”‚   â”œâ”€â”€ rag_workflow.html          # RAG workflow diagram
â”‚   â””â”€â”€ feedback_workflow.html     # Feedback loop diagram
â”œâ”€â”€ ğŸ–¼ï¸ images/
â”‚   â”œâ”€â”€ L4-diagrams.png           # Architecture diagrams
â”‚   â””â”€â”€ L4-diag-2.png             # Workflow illustrations
â”œâ”€â”€ ğŸ”§ lib/                        # External libraries
â”œâ”€â”€ ğŸ’¾ storage/                    # Vector store persistence
â”œâ”€â”€ ğŸ¤ vosk-model-*/               # Speech recognition model
â”œâ”€â”€ ğŸ› ï¸ helper.py                   # Utility functions
â”œâ”€â”€ ğŸ“‹ requirements.txt            # Dependencies
â”œâ”€â”€ ğŸ” .env                        # Environment variables
â””â”€â”€ ğŸ“– README.md                   # This file
```

## ğŸ› ï¸ Technologies

### ğŸ§  **AI & ML**
- **LLM**: Google Gemini 3 27B (Gemma)
- **Embeddings**: Google text-embedding-004
- **Speech Recognition**: VOSK (offline)
- **Document Parsing**: LlamaParse

### âš¡ **Frameworks & Libraries**
- **Workflow Engine**: LlamaIndex Workflows
- **Vector Store**: LlamaIndex VectorStoreIndex
- **UI**: Gradio for interactive interfaces
- **Async**: Python asyncio for concurrency

### ğŸ”§ **Development Tools**
- **Environment**: Jupyter Notebooks
- **Package Management**: pip + requirements.txt
- **Version Control**: Git
- **Visualization**: HTML workflow diagrams

## ğŸ“ˆ Performance

### âš¡ **Benchmarks**
| Operation | Average Time | Memory Usage |
|-----------|--------------|--------------|
| Document Parsing | 2-5 seconds | ~100MB |
| Vector Indexing | 3-8 seconds | ~200MB |
| Query Processing | 1-3 seconds | ~50MB |
| Speech Recognition | Real-time | ~150MB |

### ğŸ¯ **Optimization Tips**
- Use persistent vector storage to avoid re-indexing
- Batch process multiple documents
- Configure appropriate chunk sizes
- Monitor memory usage for large documents

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### ğŸ”„ **Development Workflow**
1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### ğŸ“ **Contribution Guidelines**
- Follow PEP 8 style guidelines
- Add tests for new features
- Update documentation
- Ensure all notebooks run successfully

### ğŸ› **Bug Reports**
Please use the [issue tracker](https://github.com/jagadeshchilla/Event-Driven-Agentic-Document-Workflows/issues) to report bugs.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸŒŸ **Star History**

[![Star History Chart](https://api.star-history.com/svg?repos=jagadeshchilla/Event-Driven-Agentic-Document-Workflows&type=Date)](https://star-history.com/#jagadeshchilla/Event-Driven-Agentic-Document-Workflows&Date)

---

<div align="center">

### ğŸš€ **Ready to build intelligent document workflows?**

[Get Started](#-quick-start) â€¢ [View Examples](#-usage-examples) â€¢ [Join Community](https://github.com/jagadeshchilla/Event-Driven-Agentic-Document-Workflows/discussions)

**Made with â¤ï¸ by [Jagadesh Chilla](https://github.com/jagadeshchilla)**

</div> 