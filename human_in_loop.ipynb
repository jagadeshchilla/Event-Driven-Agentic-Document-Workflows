{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b5ff6a-45fb-4cfa-a875-b5dbfe61b4b5",
   "metadata": {},
   "source": [
    "# Human in the Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aac9f2-6cc8-4133-b8f4-40f3c81e7a67",
   "metadata": {},
   "source": [
    "**objective**: Get feedback on answers from a human operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dffa58e-3474-4c4f-8590-2479dae67252",
   "metadata": {
    "height": 385
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Welcome\\Desktop\\ai agent\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")\n",
    "from helper import get_gemini_api_key, get_llama_cloud_api_key\n",
    "from IPython.display import display, HTML\n",
    "from helper import extract_html_content\n",
    "from llama_index.utils.workflow import draw_all_possible_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de39fa29-12ac-4b3e-9d1e-94852b096839",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54946a4f-5ab5-4519-a0fe-ecf54268476b",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "llama_cloud_api_key = get_llama_cloud_api_key()\n",
    "gemini_api_key = get_gemini_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d56f7-de48-4e1f-a288-a4c51db27251",
   "metadata": {},
   "source": [
    "## Adding a feedback loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251da79-e354-4dba-8d49-67fa7278a11d",
   "metadata": {},
   "source": [
    "Here's what you built in lesson 4:\n",
    "\n",
    "<img width=\"400\" src=\"images/L4.png\">\n",
    "\n",
    "LLMs are amazing, but they are best used to augment rather than replace a human. Your current form-filler does an excellent job figuring out what fields need to be filled in, and gets most of the fields right, but there are a couple where it needs a little help. To take care of those, you'll create a \"human in the loop\" workflow, where you can optionally provide feedback to the agent you've created and have it incorporated into the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b6dd6-9476-4b72-a7a5-9aa3ac2f2d5b",
   "metadata": {},
   "source": [
    "This is what you'll implement in this notebook:\n",
    "\n",
    "<img width=\"500\" src=\"images/L5.png\">\n",
    "\n",
    "The changes you're going to make here are:\n",
    "1. Use the `InputRequiredEvent` and `HumanResponseEvent`, new special events specifically designed to allow you to exit the workflow, and get feedback back into it.\n",
    "2. You used to have a single step which parsed your form and fired off all your questions. Since we now might loop back and ask questions several times, we don't need to parse the form every time, so we'll split up those steps. This kind of refactoring is very common as you create a more complex workflow:\n",
    "   - Your new `generate_questions` step will be triggered either by a `GenerateQuestionsEvent`, triggered by the form parser, or by a `FeedbackEvent`, which is the loop we'll take after getting feedback.\n",
    "3. `fill_in_application` will emit an `InputRequiredEvent`, and in the `external_step` you'll wait for a `HumanResponseEvent`. This will pause the whole workflow waiting for outside input.\n",
    "4. Finally, you'll use the LLM to parse the feedback and decide whether it means you should continue and output the results, or if you need to loop back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6a7149-ded2-48b0-be39-eb32a9a22cca",
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "# new!\n",
    "from llama_index.core.workflow import InputRequiredEvent, HumanResponseEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c7ef5a-0a39-41a9-adc1-8785c7f644ee",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form: str\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "    field: str\n",
    "    \n",
    "class ResponseEvent(Event):\n",
    "    response: str\n",
    "\n",
    "# new!\n",
    "class FeedbackEvent(Event):\n",
    "    feedback: str\n",
    "\n",
    "class GenerateQuestionsEvent(Event):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd98cd7-1b98-4d26-976e-d2f9d1cea668",
   "metadata": {
    "height": 2527
   },
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    \n",
    "    storage_dir = \"./storage\"\n",
    "    llm: Gemini\n",
    "    query_engine: VectorStoreIndex\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = Gemini(model=\"models/gemma-3-27b-it\", api_key=gemini_api_key)\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=\n",
    "                                                           self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=\"markdown\",\n",
    "                content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "            ).load_data(ev.resume_file)\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=GeminiEmbedding(model_name=\"models/text-embedding-004\", api_key=gemini_api_key)\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # you no longer need a query to be passed in, \n",
    "        # you'll be generating the queries instead \n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    # new - separated the form parsing from the question generation\n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> GenerateQuestionsEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=\"markdown\",\n",
    "            content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "            formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in, in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\")\n",
    "        \n",
    "        # Debug: print what the LLM returned\n",
    "        print(f\"LLM returned: '{raw_json.text}'\")\n",
    "        \n",
    "        # Try to parse JSON, with error handling\n",
    "        try:\n",
    "            # Clean the response by removing markdown code blocks if present\n",
    "            json_text = raw_json.text.strip()\n",
    "            if json_text.startswith(\"```json\"):\n",
    "                json_text = json_text[7:]  # Remove ```json\n",
    "            if json_text.endswith(\"```\"):\n",
    "                json_text = json_text[:-3]  # Remove ```\n",
    "            json_text = json_text.strip()\n",
    "            \n",
    "            fields = json.loads(json_text)[\"fields\"]\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            print(f\"Raw response: '{raw_json.text}'\")\n",
    "            # Fallback: extract fields manually or use a default list\n",
    "            fields = [\n",
    "                \"First Name\", \"Last Name\", \"Email\", \"Phone\", \"Linkedin\", \n",
    "                \"Project Portfolio\", \"Degree\", \"Graduation Date\", \n",
    "                \"Current Job Title\", \"Current Employer\", \"Technical Skills\",\n",
    "                \"Describe why you're a good fit for this position\",\n",
    "                \"Do you have 5 years of experience in React?\"\n",
    "            ]\n",
    "            print(f\"Using fallback fields: {fields}\")\n",
    "\n",
    "        await ctx.set(\"fields_to_fill\", fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "\n",
    "    # new - this step can get triggered either by GenerateQuestionsEvent or a FeedbackEvent\n",
    "    @step\n",
    "    async def generate_questions(self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent) -> QueryEvent:\n",
    "\n",
    "        # get the list of fields to fill in\n",
    "        fields = await ctx.get(\"fields_to_fill\")\n",
    "\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "            ctx.send_event(QueryEvent(\n",
    "                field=field,\n",
    "                query=question\n",
    "            ))\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.set(\"total_fields\", len(fields))\n",
    "        return\n",
    "        \n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "  \n",
    "    # new - we now emit an InputRequiredEvent\n",
    "    @step\n",
    "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return None # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\")\n",
    "\n",
    "        # new! save the result for later\n",
    "        await ctx.set(\"filled_form\", str(result))\n",
    "\n",
    "        # new! Let's get a human in the loop\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result\n",
    "        )\n",
    "\n",
    "    # new! Accept the feedback.\n",
    "    @step\n",
    "    async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\")\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if (verdict == \"OKAY\"):\n",
    "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
    "        else:\n",
    "            return FeedbackEvent(feedback=ev.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1112a0-32c6-4db7-933e-c03ff649cd8b",
   "metadata": {},
   "source": [
    "Okay! Your workflow is now ready to get some feedback, but how do we actually get it? The `InputRequiredEvent` is an event in the event stream, just like the `ProgressEvents` and `TextEvents` you've seen in lesson 2. You can intercept it the same way you did those, and use the `send_event` method on the context to send back a `HumanResponseEvent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5cce3c-dde1-43ac-b52d-a0a4c3239210",
   "metadata": {
    "height": 385
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_16900\\1964237984.py:17: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  self.llm = Gemini(model=\"models/gemma-3-27b-it\", api_key=gemini_api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 03b76af4-f332-470a-a907-c27ad99ed9cd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_16900\\1964237984.py:36: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  embed_model=GeminiEmbedding(model_name=\"models/text-embedding-004\", api_key=gemini_api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 2d73431c-c9c0-4c65-98b5-fd99dc5611a4\n",
      "LLM returned: '```json\n",
      "{\n",
      "  \"fields\": [\n",
      "    \"First Name\",\n",
      "    \"Last Name\",\n",
      "    \"Email\",\n",
      "    \"Phone\",\n",
      "    \"LinkedIn\",\n",
      "    \"Project Portfolio\",\n",
      "    \"Degree\",\n",
      "    \"Graduation Date\",\n",
      "    \"Current Job Title\",\n",
      "    \"Current Employer\",\n",
      "    \"Technical Skills\",\n",
      "    \"Describe why you’re a good fit for this position\",\n",
      "    \"Do you have 5 years of experience in React?\"\n",
      "  ]\n",
      "}\n",
      "```'\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "Here's the combined list of fields and answers, based on the provided responses:\n",
      "\n",
      "*   **First Name:** Sarah\n",
      "*   **Last Name:** Chen\n",
      "*   **Email:** sarah.chen@email.com\n",
      "*   **Phone:** Not available\n",
      "*   **LinkedIn:** linkedin.com/in/sarahchen\n",
      "*   **Project Portfolio:** EcoTrack (React, Node.js, MongoDB - featured in TechCrunch), ChatFlow (React, WebSocket - 5000+ MAU)\n",
      "*   **Degree:** Bachelor of Science in Computer Science\n",
      "*   **Graduation Date:** 2017\n",
      "*   **Current Job Title:** Senior Full Stack Developer\n",
      "*   **Current Employer:** TechFlow Solutions\n",
      "*   **Technical Skills:** React.js, Redux, Next.js, TypeScript, Vue.js, Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel.\n",
      "*   **Describe why you’re a good fit for this position:** 6+ years experience building scalable web applications, specializing in React, Node.js, and cloud architecture. Proven leadership, CI/CD implementation (40% deployment time reduction), and commitment to code quality & mentorship.\n",
      "*   **Do you have 5 years of experience in React?** Substantial experience with React.js (and related technologies) demonstrated through rebuilding a flagship product and developing customer-facing applications.\n",
      "LLM says the verdict was OKAY\n",
      "Agent complete! Here's your final result:\n",
      "Here's the combined list of fields and answers, based on the provided responses:\n",
      "\n",
      "*   **First Name:** Sarah\n",
      "*   **Last Name:** Chen\n",
      "*   **Email:** sarah.chen@email.com\n",
      "*   **Phone:** Not available\n",
      "*   **LinkedIn:** linkedin.com/in/sarahchen\n",
      "*   **Project Portfolio:** EcoTrack (React, Node.js, MongoDB - featured in TechCrunch), ChatFlow (React, WebSocket - 5000+ MAU)\n",
      "*   **Degree:** Bachelor of Science in Computer Science\n",
      "*   **Graduation Date:** 2017\n",
      "*   **Current Job Title:** Senior Full Stack Developer\n",
      "*   **Current Employer:** TechFlow Solutions\n",
      "*   **Technical Skills:** React.js, Redux, Next.js, TypeScript, Vue.js, Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel.\n",
      "*   **Describe why you’re a good fit for this position:** 6+ years experience building scalable web applications, specializing in React, Node.js, and cloud architecture. Proven leadership, CI/CD implementation (40% deployment time reduction), and commitment to code quality & mentorship.\n",
      "*   **Do you have 5 years of experience in React?** Substantial experience with React.js (and related technologies) demonstrated through rebuilding a flagship product and developing customer-facing applications.\n"
     ]
    }
   ],
   "source": [
    "w = RAGWorkflow(timeout=600, verbose=False)\n",
    "handler = w.run(\n",
    "    resume_file=\"data/fake_resume.pdf\",\n",
    "    application_form=\"data/fake_application_form.pdf\"\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        print(\"We've filled in your form! Here are the results:\\n\")\n",
    "        print(event.result)\n",
    "        # now ask for input from the keyboard\n",
    "        response = input(event.prefix)\n",
    "        handler.ctx.send_event(\n",
    "            HumanResponseEvent(\n",
    "                response=response\n",
    "            )\n",
    "        )\n",
    "\n",
    "response = await handler\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da01c5-050a-4319-b9dc-39e55cc4bd7e",
   "metadata": {},
   "source": [
    "## Using the Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6cf5f-148e-472e-9526-dac9a21364c8",
   "metadata": {},
   "source": [
    "Okay! Now let's further modify things to actually do something useful with the feedback in `generate_questions` step. This involves checking if there's feedback, and appending it to the questions. In this simple example, we're going to append the feedback to every question in case it's relevant, but a more sophisticated agent might apply it only to the fields where the feedback applied.\n",
    "\n",
    "<img width=\"500\" src=\"images/L5-use_feedback.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081d90a7-95a6-4b02-914b-6aa09accb0c5",
   "metadata": {
    "height": 2646
   },
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    \n",
    "    storage_dir = \"./storage\"\n",
    "    llm: Gemini\n",
    "    query_engine: VectorStoreIndex\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = Gemini(model=\"models/gemma-3-27b-it\", api_key=gemini_api_key)\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=\n",
    "                                                           self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=\"markdown\",\n",
    "                content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "            ).load_data(ev.resume_file)\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=GeminiEmbedding(model_name=\"models/text-embedding-004\", api_key=gemini_api_key)\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    # form parsing\n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> GenerateQuestionsEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=\"markdown\",\n",
    "            content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "            formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in, in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\")\n",
    "        \n",
    "        # Debug: print what the LLM returned\n",
    "        print(f\"LLM returned: '{raw_json.text}'\")\n",
    "        \n",
    "        # Try to parse JSON, with error handling\n",
    "        try:\n",
    "            # Clean the response by removing markdown code blocks if present\n",
    "            json_text = raw_json.text.strip()\n",
    "            if json_text.startswith(\"```json\"):\n",
    "                json_text = json_text[7:]  # Remove ```json\n",
    "            if json_text.endswith(\"```\"):\n",
    "                json_text = json_text[:-3]  # Remove ```\n",
    "            json_text = json_text.strip()\n",
    "            \n",
    "            fields = json.loads(json_text)[\"fields\"]\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            print(f\"Raw response: '{raw_json.text}'\")\n",
    "            # Fallback: extract fields manually or use a default list\n",
    "            fields = [\n",
    "                \"First Name\", \"Last Name\", \"Email\", \"Phone\", \"Linkedin\", \n",
    "                \"Project Portfolio\", \"Degree\", \"Graduation Date\", \n",
    "                \"Current Job Title\", \"Current Employer\", \"Technical Skills\",\n",
    "                \"Describe why you're a good fit for this position\",\n",
    "                \"Do you have 5 years of experience in React?\"\n",
    "            ]\n",
    "            print(f\"Using fallback fields: {fields}\")\n",
    "\n",
    "        await ctx.set(\"fields_to_fill\", fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "\n",
    "    # generate questions\n",
    "    @step\n",
    "    async def generate_questions(self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent) -> QueryEvent:\n",
    "\n",
    "        # get the list of fields to fill in\n",
    "        fields = await ctx.get(\"fields_to_fill\")\n",
    "\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "\n",
    "            # new! Is there feedback? If so, add it to the query:\n",
    "            if hasattr(ev,\"feedback\"):\n",
    "                question += f\"\"\"\n",
    "                    \\nWe previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "            \n",
    "            ctx.send_event(QueryEvent(\n",
    "                field=field,\n",
    "                query=question\n",
    "            ))\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.set(\"total_fields\", len(fields))\n",
    "        return\n",
    "        \n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "  \n",
    "    # Get feedback from the human\n",
    "    @step\n",
    "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return None # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\")\n",
    "\n",
    "        # save the result for later\n",
    "        await ctx.set(\"filled_form\", str(result))\n",
    "\n",
    "        # Fire off the feedback request\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result\n",
    "        )\n",
    "\n",
    "    # Accept the feedback when a HumanResponseEvent fires\n",
    "    @step\n",
    "    async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\")\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if (verdict == \"OKAY\"):\n",
    "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
    "        else:\n",
    "            return FeedbackEvent(feedback=ev.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230bc38-40cb-43d2-80ad-b179b96c2b82",
   "metadata": {},
   "source": [
    "Now run the workflow and give feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27001072-04b8-4459-b61b-f7cc0a441b37",
   "metadata": {
    "height": 385
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_16900\\641702230.py:17: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  self.llm = Gemini(model=\"models/gemma-3-27b-it\", api_key=gemini_api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 7678f595-0512-4135-8728-2e98a64ea74c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_16900\\641702230.py:36: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  embed_model=GeminiEmbedding(model_name=\"models/text-embedding-004\", api_key=gemini_api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id bdbaa84c-fc48-4d09-ab48-df9f09a35603\n",
      "LLM returned: '```json\n",
      "{\n",
      "  \"fields\": [\n",
      "    \"First Name\",\n",
      "    \"Last Name\",\n",
      "    \"Email\",\n",
      "    \"Phone\",\n",
      "    \"LinkedIn\",\n",
      "    \"Project Portfolio\",\n",
      "    \"Degree\",\n",
      "    \"Graduation Date\",\n",
      "    \"Current Job Title\",\n",
      "    \"Current Employer\",\n",
      "    \"Technical Skills\",\n",
      "    \"Describe why you’re a good fit for this position\",\n",
      "    \"Do you have 5 years of experience in React?\"\n",
      "  ]\n",
      "}\n",
      "```'\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "Here's the combined list of fields and answers, based on the provided responses:\n",
      "\n",
      "*   **First Name:** Sarah\n",
      "*   **Last Name:** Chen\n",
      "*   **Email:** sarah.chen@email.com\n",
      "*   **Phone:** Not available\n",
      "*   **LinkedIn:** linkedin.com/in/sarahchen\n",
      "*   **Project Portfolio:** EcoTrack (React, Node.js, MongoDB - featured in TechCrunch), ChatFlow (React, WebSocket - 5000+ MAU)\n",
      "*   **Degree:** Bachelor of Science in Computer Science\n",
      "*   **Graduation Date:** 2017\n",
      "*   **Current Job Title:** Senior Full Stack Developer\n",
      "*   **Current Employer:** TechFlow Solutions\n",
      "*   **Technical Skills:** React.js, Redux, Next.js, TypeScript, Vue.js, Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel.\n",
      "*   **Describe why you’re a good fit for this position:** 6+ years experience building scalable web applications with React, Node.js, and cloud architecture. Proven leadership, CI/CD implementation (40% deployment time reduction), and commitment to code quality & mentorship.\n",
      "*   **Do you have 5 years of experience in React?** Yes, substantial experience with React.js and related technologies, exceeding 5 years.\n",
      "LLM says the verdict was OKAY\n",
      "Agent complete! Here's your final result:\n",
      "Here's the combined list of fields and answers, based on the provided responses:\n",
      "\n",
      "*   **First Name:** Sarah\n",
      "*   **Last Name:** Chen\n",
      "*   **Email:** sarah.chen@email.com\n",
      "*   **Phone:** Not available\n",
      "*   **LinkedIn:** linkedin.com/in/sarahchen\n",
      "*   **Project Portfolio:** EcoTrack (React, Node.js, MongoDB - featured in TechCrunch), ChatFlow (React, WebSocket - 5000+ MAU)\n",
      "*   **Degree:** Bachelor of Science in Computer Science\n",
      "*   **Graduation Date:** 2017\n",
      "*   **Current Job Title:** Senior Full Stack Developer\n",
      "*   **Current Employer:** TechFlow Solutions\n",
      "*   **Technical Skills:** React.js, Redux, Next.js, TypeScript, Vue.js, Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel.\n",
      "*   **Describe why you’re a good fit for this position:** 6+ years experience building scalable web applications with React, Node.js, and cloud architecture. Proven leadership, CI/CD implementation (40% deployment time reduction), and commitment to code quality & mentorship.\n",
      "*   **Do you have 5 years of experience in React?** Yes, substantial experience with React.js and related technologies, exceeding 5 years.\n"
     ]
    }
   ],
   "source": [
    "w = RAGWorkflow(timeout=600, verbose=False)\n",
    "handler = w.run(\n",
    "    resume_file=\"data/fake_resume.pdf\",\n",
    "    application_form=\"data/fake_application_form.pdf\"\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        print(\"We've filled in your form! Here are the results:\\n\")\n",
    "        print(event.result)\n",
    "        # now ask for input from the keyboard\n",
    "        response = input(event.prefix)\n",
    "        handler.ctx.send_event(\n",
    "            HumanResponseEvent(\n",
    "                response=response\n",
    "            )\n",
    "        )\n",
    "\n",
    "response = await handler\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46b976",
   "metadata": {},
   "source": [
    "## Workflow Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfe9e8-9aed-4977-a5fc-07976b5e17a8",
   "metadata": {},
   "source": [
    "You can visualize the workflow you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9526d28-a1e9-43fe-baed-706a3c3adbc2",
   "metadata": {
    "height": 96
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflows/feedback_workflow.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <div style=\"width: 100%; height: 800px; overflow: hidden;\"> <html>\n",
       "    <head>\n",
       "        <meta charset=\"utf-8\">\n",
       "        \n",
       "            <script src=\"lib/bindings/utils.js\"></script>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
       "            \n",
       "        \n",
       "<center>\n",
       "<h1></h1>\n",
       "</center>\n",
       "\n",
       "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
       "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
       "        <link\n",
       "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
       "          rel=\"stylesheet\"\n",
       "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        />\n",
       "        <script\n",
       "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
       "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        ></script>\n",
       "\n",
       "\n",
       "        <center>\n",
       "          <h1></h1>\n",
       "        </center>\n",
       "        <style type=\"text/css\">\n",
       "\n",
       "             #mynetwork {\n",
       "                 width: 100%;\n",
       "                 height: 750px;\n",
       "                 background-color: #ffffff;\n",
       "                 border: 1px solid lightgray;\n",
       "                 position: relative;\n",
       "                 float: left;\n",
       "             }\n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "        </style>\n",
       "    </head>\n",
       "\n",
       "\n",
       "    <body>\n",
       "        <div class=\"card\" style=\"width: 100%\">\n",
       "            \n",
       "            \n",
       "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        <script type=\"text/javascript\">\n",
       "\n",
       "              // initialize global variables.\n",
       "              var edges;\n",
       "              var nodes;\n",
       "              var allNodes;\n",
       "              var allEdges;\n",
       "              var nodeColors;\n",
       "              var originalNodes;\n",
       "              var network;\n",
       "              var container;\n",
       "              var options, data;\n",
       "              var filter = {\n",
       "                  item : '',\n",
       "                  property : '',\n",
       "                  value : []\n",
       "              };\n",
       "\n",
       "              \n",
       "\n",
       "              \n",
       "\n",
       "              // This method is responsible for drawing the graph, returns the drawn network\n",
       "              function drawGraph() {\n",
       "                  var container = document.getElementById('mynetwork');\n",
       "\n",
       "                  \n",
       "\n",
       "                  // parsing and collecting nodes and edges from the python\n",
       "                  nodes = new vis.DataSet([{\"color\": \"#ADD8E6\", \"id\": \"_done\", \"label\": \"_done\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#FFA07A\", \"id\": \"StopEvent\", \"label\": \"StopEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"ask_question\", \"label\": \"ask_question\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"QueryEvent\", \"label\": \"QueryEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"ResponseEvent\", \"label\": \"ResponseEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"fill_in_application\", \"label\": \"fill_in_application\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"InputRequiredEvent\", \"label\": \"InputRequiredEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#BEDAE4\", \"id\": \"external_step\", \"label\": \"external_step\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"generate_questions\", \"label\": \"generate_questions\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"GenerateQuestionsEvent\", \"label\": \"GenerateQuestionsEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"FeedbackEvent\", \"label\": \"FeedbackEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"get_feedback\", \"label\": \"get_feedback\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"HumanResponseEvent\", \"label\": \"HumanResponseEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"parse_form\", \"label\": \"parse_form\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"ParseFormEvent\", \"label\": \"ParseFormEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"set_up\", \"label\": \"set_up\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#E27AFF\", \"id\": \"StartEvent\", \"label\": \"StartEvent\", \"shape\": \"ellipse\", \"title\": null}]);\n",
       "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"StopEvent\", \"to\": \"_done\"}, {\"arrows\": \"to\", \"from\": \"ask_question\", \"to\": \"ResponseEvent\"}, {\"arrows\": \"to\", \"from\": \"QueryEvent\", \"to\": \"ask_question\"}, {\"arrows\": \"to\", \"from\": \"fill_in_application\", \"to\": \"InputRequiredEvent\"}, {\"arrows\": \"to\", \"from\": \"InputRequiredEvent\", \"to\": \"external_step\"}, {\"arrows\": \"to\", \"from\": \"ResponseEvent\", \"to\": \"fill_in_application\"}, {\"arrows\": \"to\", \"from\": \"generate_questions\", \"to\": \"QueryEvent\"}, {\"arrows\": \"to\", \"from\": \"GenerateQuestionsEvent\", \"to\": \"generate_questions\"}, {\"arrows\": \"to\", \"from\": \"FeedbackEvent\", \"to\": \"generate_questions\"}, {\"arrows\": \"to\", \"from\": \"get_feedback\", \"to\": \"FeedbackEvent\"}, {\"arrows\": \"to\", \"from\": \"get_feedback\", \"to\": \"StopEvent\"}, {\"arrows\": \"to\", \"from\": \"HumanResponseEvent\", \"to\": \"get_feedback\"}, {\"arrows\": \"to\", \"from\": \"external_step\", \"to\": \"HumanResponseEvent\"}, {\"arrows\": \"to\", \"from\": \"parse_form\", \"to\": \"GenerateQuestionsEvent\"}, {\"arrows\": \"to\", \"from\": \"ParseFormEvent\", \"to\": \"parse_form\"}, {\"arrows\": \"to\", \"from\": \"set_up\", \"to\": \"ParseFormEvent\"}, {\"arrows\": \"to\", \"from\": \"StartEvent\", \"to\": \"set_up\"}]);\n",
       "\n",
       "                  nodeColors = {};\n",
       "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "                  for (nodeId in allNodes) {\n",
       "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
       "                  }\n",
       "                  allEdges = edges.get({ returnType: \"Object\" });\n",
       "                  // adding nodes and edges to the graph\n",
       "                  data = {nodes: nodes, edges: edges};\n",
       "\n",
       "                  var options = {\n",
       "    \"configure\": {\n",
       "        \"enabled\": false\n",
       "    },\n",
       "    \"edges\": {\n",
       "        \"color\": {\n",
       "            \"inherit\": true\n",
       "        },\n",
       "        \"smooth\": {\n",
       "            \"enabled\": true,\n",
       "            \"type\": \"dynamic\"\n",
       "        }\n",
       "    },\n",
       "    \"interaction\": {\n",
       "        \"dragNodes\": true,\n",
       "        \"hideEdgesOnDrag\": false,\n",
       "        \"hideNodesOnDrag\": false\n",
       "    },\n",
       "    \"physics\": {\n",
       "        \"enabled\": true,\n",
       "        \"stabilization\": {\n",
       "            \"enabled\": true,\n",
       "            \"fit\": true,\n",
       "            \"iterations\": 1000,\n",
       "            \"onlyDynamicEdges\": false,\n",
       "            \"updateInterval\": 50\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  network = new vis.Network(container, data, options);\n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  return network;\n",
       "\n",
       "              }\n",
       "              drawGraph();\n",
       "        </script>\n",
       "    </body>\n",
       "</html> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "isolated": true
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WORKFLOW_FILE = \"workflows/feedback_workflow.html\"\n",
    "draw_all_possible_flows(w, filename=WORKFLOW_FILE)\n",
    "html_content = extract_html_content(WORKFLOW_FILE)\n",
    "display(HTML(html_content), metadata=dict(isolated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5189c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
